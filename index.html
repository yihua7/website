<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-148984682-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-148984682-2');
  </script>
  
  <title>Yihua Huang</title>
  
  <meta name="author" content="Yihua Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="yihua.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="yihua.jpg" class="hoverZoomLink"></a>
            </td>
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Yihua Huang</name>
                </p>
                <p style="text-align:center">
                  <a href="mailto:huangyihua16@mails.ucas.ac.cn">Email</a> &nbsp/&nbsp
                  <a href="https://drive.google.com/file/d/1N4lD1FHbiebmtoVjKmKbcNF6hQXWLXw4/view?usp=sharing">CV</a>  &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/yihua-huang-0b2225245/">Linkedin</a>  &nbsp/&nbsp
                  <a href="https://github.com/yihua7/">Github</a>  &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?hl=en&user=zLil53UAAAAJ">Scholar</a>
                </p>

                <p>
                  I am a second-year PhD student at the <a href="https://xjqi.github.io/cvmi.html">CVMI Lab</a>, supervised by <a href="https://xjqi.github.io/">Xiaojuan Qi</a>. My research focuses on 3D/4D reconstruction, interaction, simulation, and editing. Prior to this, I completed my master’s degree at the <a href="http://english.ict.cas.cn/">Institute of Computing Technology</a>, part of the <a href="https://english.cas.cn/">Chinese Academy of Sciences</a>, under the supervision of Professor <a href="http://geometrylearning.com/">Lin Gao</a>. I deeply appreciate the valuable guidance and support provided by Prof. Gao during my studies. I have also had the privilege of collaborating closely with Dr. <a href="https://yanpei.me/">Yan-Pei Cao</a> and Professor <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>, both of whom significantly contributed to my academic growth. Before embarking on my master’s program, I earned my bachelor's degree from the <a href="https://english.ucas.ac.cn/">University of Chinese Academy of Sciences</a>, where I was mentored by the esteemed Professor <a href="http://vipl.ict.ac.cn/people/xlchen/">Xilin Chen</a>. Prof. Chen introduced me to my research field and taught me the foundational principles of conducting research, for which I am sincerely grateful.
                  <!-- I'm very interested in 3D reconstruction from images, 3D shape analysis, and other 3D vision problems. My research interests also include robotics, slam and federated learning. -->
                </p>   
                <p style="color:magenta">
                <!-- <em>I will join <a href="https://xjqi.github.io/">Xiaojuan Qi</a> 's team as a Ph.D student , engaged in the research of 3D reconstruction and lifelong learning. </em> -->
                </p>
              </td>
            </tr>
          </tbody>
        </table>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">          
          <tbody>
            <tr>
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                <heading>Research</heading>
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [11]&nbsp&nbsp<papertitle>Deformable Radial Kernel Splatting</papertitle>
                  <br>
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong> </a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=smnEog0AAAAJ"><strong>MingXian Lin</strong> </a>,
                  <a href="https://sunyangtian.github.io/">Yang-Tian Sun</a>,
                  <a href="https://github.com/ingra14m">Ziyi Yang</a>,
                  <a href="https://github.com/shawLyu">Xiaoyang Lyu</a><sup></sup>,
                  <a href="https://yanpei.me/">Yan-Pei Cao</a><sup>#</sup>,
                  <a href="https://xjqi.github.io/">Xiaojuan Qi</a><sup>#</sup>
                  <br>
                  <em>arXiv</em>, 2024 &nbsp
                  <br>
                  <a href="https://arxiv.org/pdf/2412.11752">paper</a> /
                  <a href="https://yihua7.github.io/DRK-web/">project page</a> /
                  <a href="https://github.com/yihua7/Deformable-Radial-Kernel-Splatting">code</a> 
                  <br>
                  We introduce Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more general and flexible framework. Through learnable radial bases with adjustable angles and scales, DRK efficiently models diverse shape primitives while enabling precise control over edge sharpness and boundary curvature
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [10]&nbsp&nbsp<papertitle>SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes</papertitle>
                  <br>
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong> </a><sup>*</sup>,
                  <a href="https://sunyangtian.github.io/">Yang-Tian Sun</a><sup>*</sup>,
                  <a href="https://github.com/ingra14m">Ziyi Yang</a><sup>*</sup>,
                  <a href="https://github.com/shawLyu">Xiaoyang Lyu</a><sup></sup>,
                  <a href="https://yanpei.me/">Yan-Pei Cao</a><sup>#</sup>,
                  <a href="https://xjqi.github.io/">Xiaojuan Qi</a><sup>#</sup>
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2312.14937">paper</a> /
                  <a href="https://yihua7.github.io/SC-GS-web/">project page</a> /
                  <a href="https://github.com/yihua7/SC-GS">code</a> 
                  <br>
                  We introduce sparse-controlled gaussian splatting to synthesize dynamic novel views. With the learned node graph of sparse control points, real-time editing can be achieved with ARAP deformation by interactive dragging of users.
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [9]&nbsp&nbsp<papertitle>Splatter a Video: Video Gaussian Representation for Versatile Processing</papertitle>
                  <br>
                  <a href="https://sunyangtian.github.io/">Yang-Tian Sun</a><sup>*</sup>,
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong> </a><sup>*</sup>,
                  <a>Lin Ma</a><sup></sup>,
                  <a href="https://github.com/shawLyu">Xiaoyang Lyu</a><sup></sup>,
                  <a href="https://yanpei.me/">Yan-Pei Cao</a>,
                  <a href="https://xjqi.github.io/">Xiaojuan Qi</a><sup>#</sup>
                  <br>
                  <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2406.13870">paper</a> /
                  <a href="https://sunyangtian.github.io/spatter_a_video_web/">project page</a> /
                  <a href="https://github.com/SunYangtian/Splatter_A_Video">code</a> 
                  <br>
                  We introduce a novel explicit 3D representation—video Gaussian representation—that embeds a video into 3D Gaussians, enabling tracking, consistent video depth and feature refinement, motion and appearance editing, and stereoscopic video generation.
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [8]&nbsp&nbsp<papertitle>Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting</papertitle>
                  <br>
                  <a href="https://github.com/ingra14m">Ziyi Yang</a>,
                  <a>Xinyu Gao</a>,
                  <a href="https://sunyangtian.github.io/">Yang-Tian Sun</a>,
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong> </a>,
                  <a href="https://github.com/shawLyu">Xiaoyang Lyu</a><sup></sup>,
                  <a>Wen Zhou</a>,
                  <a>Shaohui Jiao</a>,
                  <a href="https://xjqi.github.io/">Xiaojuan Qi</a><sup>#</sup>
                  <a>Xiaogang Jin</a><sup>#</sup>
                  <br>
                  <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024 &nbsp
                  <br>
                  <a href="https://ingra14m.github.io/Spec-Gaussian-website/file/Spec-Gaussian-nips24.pdf">paper</a> /
                  <a href="https://ingra14m.github.io/Spec-Gaussian-website/">project page</a> /
                  <a href="https://github.com/ingra14m/Spec-Gaussian">code</a> 
                  <br>
                  We introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian.
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [7]&nbsp&nbsp<papertitle>3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting</papertitle>
                  <br>
                  <a href="https://github.com/shawLyu">Xiaoyang Lyu</a><sup></sup>,
                  <a href="https://sunyangtian.github.io/">Yang-Tian Sun</a>,
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong> </a>,
                  <a>Xiuzhe Wu</a>,
                  <a href="https://github.com/ingra14m">Ziyi Yang</a>,
                  <a>Yilun Chen</a>,
                  <a>Jiangmiao Pang</a>,
                  <a href="https://xjqi.github.io/">Xiaojuan Qi</a><sup>#</sup>
                  <br>
                  <em>ACM SIGGRAPH 2024 Conference Proceedings (SIGGRAPH)</em>, 2024 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2404.00409">paper</a>
                  <br>
                  We introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians.
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [6]&nbsp&nbsp<papertitle>NeRF-Texture: Synthesizing Neural Radiance Field Textures</papertitle>
                  <br>
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong></a>,
                  <a href="https://yanpei.me/">Yan-Pei Cao</a>,
                  <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>,
                  <a>Ying Shan</a>,
                  <a href="http://geometrylearning.com/">Lin Gao</a>
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2024</em> &nbsp
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10489854">paper</a> /
                  <a href="https://yihua7.github.io/NeRF-Texture-web/">project page</a> /
                  <a href="https://github.com/yihua7/NeRF-Texture">code</a> 
                  <br>
                  We propose an algorithm to synthesize NeRF textures on arbitrary manifolds. By using a patch-matching method on curved surfaces, we can smoothly quilt texture patches on mesh surfaces. We create a multi-resolution pyramid for a fast patch-matching process. By incorporating a reflection network, we preserve high-frequency view-dependent features such as highlights and mirror reflections in the final synthesized results.
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [5]&nbsp&nbsp<papertitle>NeRF-Texture: Texture Synthesis with Neural Radiance Fields</papertitle>
                  <br>
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong></a>,
                  <a href="https://yanpei.me/">Yan-Pei Cao</a>,
                  <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>,
                  <a>Ying Shan</a>,
                  <a href="http://geometrylearning.com/">Lin Gao</a>
                  <br>
                  <em>ACM SIGGRAPH 2023 Conference Proceedings (SIGGRAPH), 2023</em> &nbsp
                  <br>
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3588432.3591484">paper</a> /
                  <a href="https://yihua7.github.io/NeRF-Texture-web/">project page</a> /
                  <a href="https://github.com/yihua7/NeRF-Texture">code</a> 
                  <br>
                  We introduce a NeRF-based system to acquire, synthesize, map, and relight textures from real-world textures. A novel coarse-fine disentangling representation is proposed to model meso-structures of textures. Acquired textures are synthesized by an implicit patch-matching algorithm.
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [4]&nbsp&nbsp<papertitle>StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning</papertitle>
                  <br>
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong></a>,
                  <a>Yue He</a>,
                  <a href="http://people.geometrylearning/yyj/">Yu-Jie Yuan</a>,
                  <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>,
                  <a href="http://geometrylearning.com/">Lin Gao</a>
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2205.12183">arxiv</a> /
                  <a href="http://geometrylearning.com/StylizedNeRF/">project page</a> /
                  <a href="https://github.com/IGLICT/StylizedNeRF">code</a> 
                  <br>
                  We propose a novel mutual learning framework for 3D scene stylization that combines a 2D image stylization network and NeRF to fuse the stylization ability of 2D stylization network with the 3D consistency of NeRF.
              </td>
            </tr>
          
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                [3]&nbsp&nbsp<papertitle>Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data</papertitle>
                <br>
                <a href="https://yutinghe20.github.io/YutingHe/">Yuting He</a>,
                <a href="https://people.ucas.ac.cn/~yqchen">Yiqiang Chen</a>,
                <a>XiaoDong Yang</a>,
                <a>Hanchao Yu</a>,
                <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong></a>,
                <a>Yang Gu</a>
                <br>
                <em>IEEE Transactions on Big Data (TBD)</em>, 2022 &nbsp
                <br>
                <a href="https://www.computer.org/csdl/journal/bd/5555/01/09826416/1EVdvuSiENO">paper</a>
                <br>
                We propose a Selective Self-Distillation method for Federated learning (FedSSD), which imposes adaptive constraints on the local updates by self-distilling the global model's knowledge and selectively weighting it by evaluating the credibility at both the class and sample level.
              </td>
            </tr>
            
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [2]&nbsp&nbsp<papertitle>Neural Radiance Fields from Sparse RGB-D Images for High-Quality View Synthesis</papertitle>
                  <br>
                  <a href="http://people.geometrylearning/yyj/">Yu-Jie Yuan</a>,
                  <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>,
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong></a>,
                  <a href="https://www.graphics.rwth-aachen.de/person/3/">Leif Kobbelt</a>,
                  <a href="http://geometrylearning.com/">Lin Gao</a>,
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI)</em>, 2022 &nbsp
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9999509">paper</a> /
                  <a href="http://geometrylearning.com/rgbdnerf/">project page</a>
                  <!-- <a href="https://github.com/IGLICT/StylizedNeRF">code</a>  -->
                  <br>
                  We introduce a novel NeRF reconstruction method using RGB-D inputs from a consumer-level device (iPad), which enables high-quality reconstruction from sparse inputs. Experiments show that the proposed method achieves state-of-the-art novel view synthesis quality in this case of sparse RGB-D inputs.
              </td>
            </tr>
          
            <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
              <td style="padding-top:0px;padding-bottom:10px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                  [1]&nbsp&nbsp<papertitle>Multiscale Mesh Deformation Component Analysis with Attention-based Autoencoders</papertitle>
                  <br>
                  <a href="http://people.geometrylearning.com/~jieyang/">Jie Yang</a>,
                  <a href="http://geometrylearning.com/">Lin Gao</a>,
                  <a href="https://qytan.com/">Qingyang Tan</a>,
                  <a href="https://yihua7.github.io/website/"><strong>Yihua Huang</strong></a>,
                  <a>Shihong Xia</a>
                  <a href="https://users.cs.cf.ac.uk/Yukun.Lai/">Yu-Kun Lai</a>
                  <br>
                  <em>IEEE Transactions on Visualization and Computer Graphics (TVCG)</em>, 2021 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2012.02459">arxiv</a>
                  <br>
                  We propose a novel method to exact multiscale deformation components automatically with a stacked attention-based autoencoder.
              </td>
            </tr>

            <!-- <tr>
              <td style="padding-top:0px;padding-bottom:20px;padding-left:20px;padding-right:20px;width:75%;vertical-align:middle">
                <heading>Services</heading>
              </td>
            </tr>
            <p>Paper reviewer: CVPR</p> -->

          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <heading>Services</heading>
                <p>
                  <strong>Internships:</strong> Tencent (2023 Summer) <br>
                  <strong>Paper reviewer:</strong> CVPR, ICCV, NeurIPS, ICLR, ECCV, TVCG, ACCV, Parcific Graphics, Virtual Reality, Computer & Graphics<br>
                  <strong>Talk speaker:</strong> Deep Blue College 2023, Graphics And Mixed Environment Seminar (GAMES) 2022<br>
                </p>
              </td>
            </tr>
          </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Kudos to <a href="https://jonbarron.info/">Dr. Jon Barron</a> for sharing his website template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
